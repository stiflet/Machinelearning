{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328f7ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasmortensen/Data/princessml/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "# 1) Load datasets\n",
    "az = load_dataset('mteb/amazon_polarity')\n",
    "rt = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "\n",
    "def to_tf_dataset(hfds, text_key=\"text\", label_key=\"label\", batch_size=64, shuffle=False):\n",
    "    # Convert HF columns to plain Python lists / numpy\n",
    "    texts = list(hfds[text_key])\n",
    "    labels = np.array(hfds[label_key], dtype=np.int32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(20000, len(texts)), seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_movie = to_tf_dataset(rt[\"train\"], shuffle=True)\n",
    "val_movie  = to_tf_dataset(rt[\"validation\"], shuffle=False)\n",
    "test_movie  = to_tf_dataset(rt[\"test\"], shuffle=False)\n",
    "\n",
    "split = az[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "az = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": split[\"test\"],\n",
    "    \"test\": az[\"test\"],\n",
    "})\n",
    "\n",
    "train_ds = to_tf_dataset(az[\"train\"], shuffle=True)\n",
    "val_ds   = to_tf_dataset(az[\"validation\"], shuffle=False)\n",
    "test_ds  = to_tf_dataset(az[\"test\"], shuffle=False)\n",
    "\n",
    "best_cfg = {\"rnn_type\":\"lstm\",\"embed_dim\":128, \"rnn_units\":256, \"dropout\":0.4, \"lr\":1e-3, \"bidirectional\":True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af202f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(\n",
    "    rnn_type=\"gru\",\n",
    "    vocab_size=20000,\n",
    "    seq_len=50,\n",
    "    embed_dim=128,\n",
    "    rnn_units=128,\n",
    "    dropout=0.3,\n",
    "    lr=1e-3,\n",
    "    bidirectional=False,\n",
    "):\n",
    "    # TextVectorization layer\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=seq_len\n",
    "    )\n",
    "\n",
    "    # Adapt vectorizer on training text only\n",
    "    train_text_only = rt[\"train\"][\"text\"]\n",
    "    vectorizer.adapt(train_text_only)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "    x = vectorizer(inputs)\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(x)\n",
    "\n",
    "    RNN = tf.keras.layers.GRU if rnn_type.lower() == \"gru\" else tf.keras.layers.LSTM\n",
    "    rnn_layer = RNN(rnn_units)\n",
    "\n",
    "    if bidirectional:\n",
    "        x = tf.keras.layers.Bidirectional(rnn_layer)(x)\n",
    "    else:\n",
    "        x = rnn_layer(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c739d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(config,epochs,train_ds,val_ds):\n",
    "    model = build_rnn_model(**config)\n",
    "    cb = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=2, restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "    print('starting training ...')\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cb, verbose=2)\n",
    "    val_best = max(hist.history[\"val_accuracy\"])\n",
    "    return model, hist, val_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b49071",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = tf.keras.models.load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c636b0",
   "metadata": {},
   "source": [
    "## train movie model initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training ...\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "movie_model,hist,val_best = train_one(config=best_cfg,epochs=10,train_ds=train_movie,val_ds=val_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa1101",
   "metadata": {},
   "source": [
    "## Movie model on amazon data (initial, no training/finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07169319",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_ds\n",
    "y_prob = movie_model.predict(test_ds)\n",
    "\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"f1 (macro):\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"recall (macro):\", recall_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"precision (macro):\", precision_score(y_true, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6743ac1",
   "metadata": {},
   "source": [
    "## finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f6010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, None), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     model.load_weights(best_weights_path)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model,history\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m model, hist = \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mfinetune\u001b[39m\u001b[34m(model, train_ds, val_ds, limit, epochs, batch_size, shuffle, seed, monitor, mode, best_weights_path, **fit_kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m ckpt = tf.keras.callbacks.ModelCheckpoint(\n\u001b[32m     27\u001b[39m     filepath=best_weights_path,\n\u001b[32m     28\u001b[39m     monitor=monitor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     verbose=\u001b[32m0\u001b[39m,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m callbacks = \u001b[38;5;28mlist\u001b[39m(fit_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m model.load_weights(best_weights_path)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model,history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data/princessml/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data/princessml/.venv/lib/python3.13/site-packages/keras/src/layers/preprocessing/text_vectorization.py:542\u001b[39m, in \u001b[36mTextVectorization._preprocess\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs.shape.rank > \u001b[32m1\u001b[39m:\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inputs.shape[-\u001b[32m1\u001b[39m] != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    543\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    544\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmust be 1. Received: inputs.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    546\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs.shape.rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    547\u001b[39m         )\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    549\u001b[39m         inputs = tf.squeeze(inputs, axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, None), dtype=string)"
     ]
    }
   ],
   "source": [
    "def finetune(\n",
    "    model,\n",
    "    train_ds,\n",
    "    *,\n",
    "    val_ds=None,\n",
    "    limit=1000,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    best_weights_path=\"best.weights.h5\",\n",
    "    **fit_kwargs,\n",
    "):\n",
    "    train_ds = train_ds.take(limit)\n",
    "    if shuffle:\n",
    "        train_ds = train_ds.shuffle(min(limit, 10_000), seed=seed, reshuffle_each_iteration=True)\n",
    "    train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    if val_ds is None:\n",
    "        return model.fit(train_ds, epochs=epochs, **fit_kwargs)\n",
    "\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=best_weights_path,\n",
    "        monitor=monitor,\n",
    "        mode=mode,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    callbacks = list(fit_kwargs.pop(\"callbacks\", []) or [])\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks + [ckpt],\n",
    "        **fit_kwargs,\n",
    "    )\n",
    "\n",
    "    model.load_weights(best_weights_path)\n",
    "    return model,history\n",
    "\n",
    "model, hist = finetune(movie_model, train_ds=train_ds, val_ds=val_ds, limit=250, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ed453",
   "metadata": {},
   "source": [
    "## train new model on amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98150b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_model,hist,val_best = train_one(config=best_cfg,epochs=10,train_ds=train_ds,val_ds=val_ds)\n",
    "val_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "princessml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
