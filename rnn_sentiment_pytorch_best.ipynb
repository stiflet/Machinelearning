{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# RNN Sentiment Analysis (PyTorch)\nRotten Tomatoes • Model selection + Uni vs Bi + Embedding stability (vs GloVe)\n\nThis notebook implements a strong RNN-based baseline using **(Bi)GRU/LSTM + Attention pooling**.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 1: Setup + data\n# =========================\nfrom datasets import load_dataset\nimport re, math, random, time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Repro\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nrt = load_dataset(\"rotten_tomatoes\")\n\n# TextVectorization-lignende standardisering: lower + strip punctuation\n# (Matcher ideen \"lower_and_strip_punctuation\" + whitespace split)\n_punct_re = re.compile(r\"[^\\w\\s']\")  # bevar apostrof i \"don't\"\n_space_re = re.compile(r\"\\s+\")\n\ndef standardize_tf_like(s: str) -> str:\n    s = s.lower()\n    s = _punct_re.sub(\" \", s)\n    s = _space_re.sub(\" \", s).strip()\n    return s\n\ndef tokenize_tf_like(s: str):\n    return standardize_tf_like(s).split()\n\nfrom collections import Counter\n\ndef build_vocab_tf_like(texts, max_tokens=20000, min_freq=1):\n    \"\"\"\n    TF-lignende:\n    - special tokens: pad=0, oov=1\n    - sortér efter frekvens desc, og alfabetisk ved ties (stabilt)\n    \"\"\"\n    c = Counter()\n    for t in texts:\n        c.update(tokenize_tf_like(t))\n    items = [(w, f) for w, f in c.items() if f >= min_freq]\n    items.sort(key=lambda x: (-x[1], x[0]))  # freq desc, alpha asc\n\n    stoi = {\"<pad>\": 0, \"<unk>\": 1}\n    itos = [\"<pad>\", \"<unk>\"]\n    for w, _ in items:\n        if w in stoi:\n            continue\n        if len(itos) >= max_tokens:\n            break\n        stoi[w] = len(itos)\n        itos.append(w)\n    return stoi, itos\n\ndef encode(text, stoi, seq_len=60):\n    toks = tokenize_tf_like(text)\n    ids = [stoi.get(tok, 1) for tok in toks]  # 1 = <unk>\n    if len(ids) < seq_len:\n        ids = ids + [0] * (seq_len - len(ids))\n    else:\n        ids = ids[:seq_len]\n    return np.asarray(ids, dtype=np.int64)\n\nclass RTDataset(Dataset):\n    def __init__(self, split, stoi, seq_len=60):\n        self.texts = split[\"text\"]\n        self.labels = split[\"label\"]\n        self.stoi = stoi\n        self.seq_len = seq_len\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        x = encode(self.texts[idx], self.stoi, self.seq_len)\n        y = float(self.labels[idx])\n        return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n\ndef make_loaders(vocab_size=20000, seq_len=60, batch_size=64):\n    stoi, itos = build_vocab_tf_like(rt[\"train\"][\"text\"], max_tokens=vocab_size)\n    train_ds = RTDataset(rt[\"train\"], stoi, seq_len)\n    val_ds   = RTDataset(rt[\"validation\"], stoi, seq_len)\n    test_ds  = RTDataset(rt[\"test\"], stoi, seq_len)\n\n    g = torch.Generator()\n    g.manual_seed(SEED)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=g, num_workers=0)\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n    return train_loader, val_loader, test_loader, stoi, itos\n\ntrain_loader, val_loader, test_loader, stoi, itos = make_loaders()\nprint(\"Vocab:\", len(itos))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 2: Model (GRU/LSTM + Attention Pooling)\n# =========================\nclass AttentionPool(nn.Module):\n    \"\"\"\n    Additiv attention over tidssteps.\n    Input: H (B, T, D)\n    Output: pooled (B, D)\n    \"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.proj = nn.Linear(dim, dim)\n        self.v = nn.Linear(dim, 1, bias=False)\n\n    def forward(self, H, mask=None):\n        scores = self.v(torch.tanh(self.proj(H))).squeeze(-1)  # (B, T)\n        if mask is not None:\n            scores = scores.masked_fill(~mask, -1e9)\n        w = torch.softmax(scores, dim=1)  # (B, T)\n        pooled = (H * w.unsqueeze(-1)).sum(dim=1)  # (B, D)\n        return pooled, w\n\nclass RNNClassifier(nn.Module):\n    def __init__(\n        self,\n        rnn_type=\"gru\",\n        vocab_size=20000,\n        embed_dim=200,\n        hidden_size=256,\n        num_layers=1,\n        dropout=0.35,\n        bidirectional=True,\n        pad_idx=0\n    ):\n        super().__init__()\n        self.rnn_type = rnn_type.lower()\n        self.bidirectional = bidirectional\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n\n        rnn_cls = nn.GRU if self.rnn_type == \"gru\" else nn.LSTM\n        self.rnn = rnn_cls(\n            input_size=embed_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=0.0 if num_layers == 1 else dropout\n        )\n\n        out_dim = hidden_size * (2 if bidirectional else 1)\n        self.attn = AttentionPool(out_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(out_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        emb = self.embedding(x)          # (B, T, E)\n        H, _ = self.rnn(emb)             # (B, T, D)\n        mask = (x != 0)                  # pad-mask\n        pooled, attn_w = self.attn(H, mask=mask)\n        logits = self.mlp(pooled).squeeze(1)  # (B,)\n        return logits, attn_w\n\ndef count_params(m):\n    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 3: Train/eval + early stopping\n# =========================\ndef acc_from_logits(logits, y):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= 0.5).float()\n    return (preds == y).float().mean().item()\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion):\n    model.eval()\n    losses, accs = [], []\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        logits, _ = model(x)\n        loss = criterion(logits, y)\n        losses.append(loss.item())\n        accs.append(acc_from_logits(logits, y))\n    return float(np.mean(losses)), float(np.mean(accs))\n\ndef train_one_epoch(model, loader, optimizer, criterion, grad_clip=1.0, use_amp=True):\n    model.train()\n    losses, accs = [], []\n    scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.cuda.amp.autocast(enabled=(use_amp and device.type == \"cuda\")):\n            logits, _ = model(x)\n            loss = criterion(logits, y)\n\n        scaler.scale(loss).backward()\n        if grad_clip is not None:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        scaler.step(optimizer)\n        scaler.update()\n\n        losses.append(loss.item())\n        accs.append(acc_from_logits(logits, y))\n\n    return float(np.mean(losses)), float(np.mean(accs))\n\ndef fit(config, epochs=12, batch_size=64, patience=2, verbose=False):\n    train_loader, val_loader, test_loader, stoi, itos = make_loaders(\n        vocab_size=config[\"vocab_size\"],\n        seq_len=config[\"seq_len\"],\n        batch_size=batch_size\n    )\n\n    model = RNNClassifier(\n        rnn_type=config[\"rnn_type\"],\n        vocab_size=config[\"vocab_size\"],\n        embed_dim=config[\"embed_dim\"],\n        hidden_size=config[\"hidden_size\"],\n        num_layers=config[\"num_layers\"],\n        dropout=config[\"dropout\"],\n        bidirectional=config[\"bidirectional\"]\n    ).to(device)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"lr\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    hist = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n    best_val_loss = float(\"inf\")\n    best_state = None\n    bad = 0\n\n    for ep in range(1, epochs+1):\n        tr_l, tr_a = train_one_epoch(model, train_loader, optimizer, criterion,\n                                     grad_clip=config[\"grad_clip\"], use_amp=True)\n        va_l, va_a = evaluate(model, val_loader, criterion)\n\n        hist[\"train_loss\"].append(tr_l); hist[\"train_acc\"].append(tr_a)\n        hist[\"val_loss\"].append(va_l);   hist[\"val_acc\"].append(va_a)\n\n        if va_l < best_val_loss - 1e-5:\n            best_val_loss = va_l\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                break\n\n        if verbose:\n            print(f\"ep {ep:02d} | tr loss {tr_l:.4f} acc {tr_a:.3f} | va loss {va_l:.4f} acc {va_a:.3f}\")\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    return model, hist, (train_loader, val_loader, test_loader, stoi, itos)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Hyperparameter search + visualisering\nVi laver et kontrolleret sweep over hidden size, dropout og learning rate på en UNI-model.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 4: Hyperparameter search + visualisering\n# =========================\nbase = dict(\n    rnn_type=\"gru\",\n    vocab_size=20000,\n    seq_len=60,\n    embed_dim=200,\n    hidden_size=256,\n    num_layers=1,\n    dropout=0.35,\n    bidirectional=False,   # UNI for fair comparison later\n    lr=2e-3,\n    weight_decay=1e-2,\n    grad_clip=1.0,\n)\n\ngrid = []\nfor hidden in [128, 256, 384]:\n    for dropout in [0.25, 0.35, 0.45]:\n        for lr in [1e-3, 2e-3]:\n            cfg = dict(base)\n            cfg[\"hidden_size\"] = hidden\n            cfg[\"dropout\"] = dropout\n            cfg[\"lr\"] = lr\n            grid.append(cfg)\n\nresults = []\nhistories = []\n\nfor i, cfg in enumerate(grid):\n    model, hist, pack = fit(cfg, epochs=12, batch_size=64, patience=2, verbose=False)\n    best_val_acc = max(hist[\"val_acc\"])\n    best_val_loss = min(hist[\"val_loss\"])\n    results.append({\n        \"run\": i,\n        \"hidden\": cfg[\"hidden_size\"],\n        \"dropout\": cfg[\"dropout\"],\n        \"lr\": cfg[\"lr\"],\n        \"best_val_acc\": best_val_acc,\n        \"best_val_loss\": best_val_loss,\n        \"params\": count_params(model)\n    })\n    histories.append(hist)\n\ndf = pd.DataFrame(results).sort_values([\"best_val_acc\",\"best_val_loss\"], ascending=[False, True]).reset_index(drop=True)\ndf.head(10)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Visualiser hyperparameter-effekt: Val accuracy pr run\nplt.figure()\nplt.plot(df[\"best_val_acc\"].values, marker=\"o\")\nplt.xlabel(\"Run (sorted by best val acc)\")\nplt.ylabel(\"Best val accuracy\")\nplt.grid(True)\nplt.show()\n\n# Heatmap-ish: groupby hidden/dropout og se gennemsnit val acc\npivot = df.pivot_table(index=\"hidden\", columns=\"dropout\", values=\"best_val_acc\", aggfunc=\"mean\")\nprint(pivot)\n\nplt.figure()\nplt.imshow(pivot.values, aspect=\"auto\")\nplt.xticks(range(pivot.shape[1]), pivot.columns)\nplt.yticks(range(pivot.shape[0]), pivot.index)\nplt.xlabel(\"Dropout\")\nplt.ylabel(\"Hidden size\")\nplt.title(\"Mean best val acc (UNI-GRU)\")\nplt.colorbar()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Train best UNI + test + learning curves"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 5: Træn endelig \"bedste UNI\" + test\n# =========================\nbest_uni = df.iloc[0].to_dict()\n\nbest_cfg_uni = None\nfor cfg in grid:\n    if cfg[\"hidden_size\"] == best_uni[\"hidden\"] and cfg[\"dropout\"] == best_uni[\"dropout\"] and cfg[\"lr\"] == best_uni[\"lr\"]:\n        best_cfg_uni = cfg\n        break\n\nuni_model, uni_hist, uni_pack = fit(best_cfg_uni, epochs=12, batch_size=64, patience=2, verbose=True)\ntrain_loader, val_loader, test_loader, uni_stoi, uni_itos = uni_pack\n\ncriterion = nn.BCEWithLogitsLoss()\ntest_loss, test_acc = evaluate(uni_model, test_loader, criterion)\nprint(\"UNI Test acc:\", test_acc)\nprint(\"UNI Params:\", count_params(uni_model))\n\nplt.figure()\nplt.plot(uni_hist[\"train_loss\"], label=\"train_loss\")\nplt.plot(uni_hist[\"val_loss\"], label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend(); plt.grid(True); plt.show()\n\nplt.figure()\nplt.plot(uni_hist[\"train_acc\"], label=\"train_acc\")\nplt.plot(uni_hist[\"val_acc\"], label=\"val_acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend(); plt.grid(True); plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Uni vs Bi comparison (value of bidirectionality)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 6: Bi-directional sammenligning (fair sammenligning)\n# =========================\nbest_cfg_bi = dict(best_cfg_uni)\nbest_cfg_bi[\"bidirectional\"] = True\n\nbi_model, bi_hist, bi_pack = fit(best_cfg_bi, epochs=12, batch_size=64, patience=2, verbose=True)\n_, _, bi_test_loader, bi_stoi, bi_itos = bi_pack\n\nbi_test_loss, bi_test_acc = evaluate(bi_model, bi_test_loader, criterion)\nprint(\"BI Test acc:\", bi_test_acc)\nprint(\"BI Params:\", count_params(bi_model))\n\nplt.figure()\nplt.plot(uni_hist[\"val_acc\"], label=\"UNI val_acc\")\nplt.plot(bi_hist[\"val_acc\"], label=\"BI val_acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Val accuracy\")\nplt.legend(); plt.grid(True); plt.show()\n\ndelta_acc = bi_test_acc - test_acc\ndelta_params = count_params(bi_model) - count_params(uni_model)\nprint(\"Δ test acc:\", delta_acc)\nprint(\"Δ params:\", delta_params)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Constructed examples where 'future context' matters"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 7: Eksempler hvor \"end-of-sentence\" betyder meget\n# =========================\n@torch.no_grad()\ndef predict_probs(model, texts, stoi, seq_len):\n    model.eval()\n    X = np.stack([encode(t, stoi, seq_len) for t in texts])\n    x = torch.from_numpy(X).to(device)\n    logits, _ = model(x)\n    return torch.sigmoid(logits).detach().cpu().numpy()\n\nexamples = [\n    \"I thought the movie was great at first, but it isn't.\",\n    \"This is not a good film despite the talented cast.\",\n    \"The acting seems brilliant, until the ending ruins it.\",\n    \"What a wonderful idea, executed so poorly.\",\n    \"I laughed a lot, mostly at how bad it was.\",\n    \"It looks promising although it never delivers.\",\n    \"The plot is clever, not.\",\n    \"This is the kind of film you recommend to your enemies.\",\n]\n\nseq_len = best_cfg_uni[\"seq_len\"]\n\nuni_p = predict_probs(uni_model, examples, uni_stoi, seq_len)\nbi_p  = predict_probs(bi_model,  examples, bi_stoi,  seq_len)\n\nfor t, up, bp in zip(examples, uni_p, bi_p):\n    print(f\"Text: {t}\")\n    print(f\"  UNI prob(pos): {float(up):.3f}\")\n    print(f\"  BI  prob(pos): {float(bp):.3f}\\n\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Embedding stability vs GloVe (dimension matched via PCA)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# =========================\n# Cell 8: Embedding stability (vs GloVe) + dimension match + cosine + nearest neighbors\n# =========================\n!pip -q install gensim\n\nimport gensim.downloader as api\nfrom numpy.linalg import norm\nfrom sklearn.decomposition import PCA\n\nglove = api.load(\"glove-twitter-25\")  # 25-dim\n\nmodel_for_emb = bi_model\nstoi_for_emb  = bi_stoi\nitos_for_emb  = bi_itos\n\nE = model_for_emb.embedding.weight.detach().cpu().numpy()  # (V, embed_dim)\n\ndef cosine(a, b):\n    return float(np.dot(a, b) / (norm(a)*norm(b) + 1e-12))\n\ndef get_vec(word):\n    idx = stoi_for_emb.get(word, None)\n    if idx is None:\n        return None\n    return E[idx]\n\ntarget_words = [\"plot\", \"acting\", \"cheap\", \"affordable\", \"excellent\", \"waste\"]\nwords = [w for w in target_words if (w in stoi_for_emb) and (w in glove)]\nprint(\"Using words:\", words)\n\nshared = [w for w in itos_for_emb if (w in glove) and (w in stoi_for_emb)]\nshared = shared[:3000]\n\nX_model = np.stack([get_vec(w) for w in shared])\npca = PCA(n_components=25, random_state=SEED).fit(X_model)\nE25 = pca.transform(E)\n\ndef vec25(word):\n    return E25[stoi_for_emb[word]]\n\npairs = [\n    (\"cheap\", \"affordable\"),\n    (\"plot\", \"acting\"),\n    (\"plot\", \"cheap\"),\n    (\"acting\", \"waste\"),\n    (\"excellent\", \"waste\"),\n    (\"plot\", \"excellent\"),\n]\n\nfor a,b in pairs:\n    if a in words and b in words:\n        sim_model = cosine(vec25(a), vec25(b))\n        sim_glove = cosine(glove[a], glove[b])\n        print(f\"\\n{a} vs {b}\")\n        print(\"  Model cosine:\", round(sim_model, 3))\n        print(\"  GloVe cosine:\", round(sim_glove, 3))\n\ndef top_neighbors(word, topk=10):\n    idx = stoi_for_emb[word]\n    v = E25[idx]\n    sims = []\n    for i, w in enumerate(itos_for_emb):\n        if i == idx or w in (\"<pad>\",\"<unk>\"):\n            continue\n        sims.append((w, cosine(v, E25[i])))\n    sims.sort(key=lambda x: x[1], reverse=True)\n    return sims[:topk]\n\nfor w in words:\n    print(f\"\\nNearest neighbors in learned embedding for '{w}':\")\n    for n, s in top_neighbors(w, topk=10):\n        print(\" \", n, round(s, 3))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}